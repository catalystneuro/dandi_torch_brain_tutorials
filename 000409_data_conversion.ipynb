{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772e1f4d-d0b9-4f6e-8430-513edbc51fb7",
   "metadata": {},
   "source": [
    "# IBL - Brain Wide Map\n",
    "\n",
    "References:\n",
    "- [Dandiset](https://dandiarchive.org/dandiset/000409/draft)\n",
    "- [explore existing sessions](https://viz.internationalbrainlab.org/app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c6ec14-014c-4b1f-9b3c-e6198b287ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m venv .venv && \\\n",
    "source .venv/bin/activate && \\\n",
    "pip install -r requirements.txt && \\\n",
    "python -m ipykernel install --user --name=.venv --display-name \"Python (.venv)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d9332c-2d0d-4cc3-a025-9f3ff4037cc4",
   "metadata": {},
   "source": [
    "## Create a Pipeline\n",
    "\n",
    "Brainsets are created using [Pipelines](https://brainsets.readthedocs.io/en/latest/concepts/create_pipeline.html). You can check the implementation details for the IBL Pipeline in `pipeline.py`.\n",
    "\n",
    "This Pipeline accepts the following arguments:\n",
    "- **interval_ref_time**: the intra-trial event to be used as a reference starting time for each interval chunk\n",
    "- **interval_max_duration**: the maximum duration for each interval chunk\n",
    "\n",
    "Intervals are chunks of time along a session which mark the periods of time to be used by training, validation and test data. The Pipeline code will automatically create these splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313ffa68-f35b-4114-9b18-1a2d9a7a67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "from pipeline import Pipeline\n",
    "\n",
    "# Define directories\n",
    "raw_dir = Path(\"./ibl_nwb\").resolve()\n",
    "processed_dir = Path(\"./ibl_processed\").resolve()\n",
    "\n",
    "# Create args namespace (simulating command-line arguments)\n",
    "args = Namespace(\n",
    "    interval_ref_time=\"gabor_stimulus_onset_time\",\n",
    "    interval_max_duration=1.0,\n",
    "    redownload=False,\n",
    "    reprocess=True,\n",
    ")\n",
    "\n",
    "# Instantiate the pipeline\n",
    "pipeline = Pipeline(\n",
    "    raw_dir=raw_dir,\n",
    "    processed_dir=processed_dir,\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "# Get the manifest\n",
    "manifest = Pipeline.get_manifest(raw_dir, args)\n",
    "manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d33a9d-f239-4ca6-818e-7aad994f83be",
   "metadata": {},
   "source": [
    "## Process sessions\n",
    "\n",
    "Now we are ready to process all sessions. This will automatically:\n",
    "- download the nwb files from DANDI, to the `ibl_nwb` folder\n",
    "- extract the relevant content from nwb files\n",
    "- save the results as a Brainset, to the `ibl_processed` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21004e09-b1e1-412e-82d6-ac0a839f4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process sessions\n",
    "for manifest_item in manifest.itertuples():\n",
    "    print(f\"Processing session: {manifest_item.Index}\")\n",
    "    pipeline._run_item(manifest_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ef2ffd-3ed6-4e2a-a5aa-d859a12d0017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_brain",
   "language": "python",
   "name": "torch_brain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
